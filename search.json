[
  {
    "objectID": "posts/outl.html",
    "href": "posts/outl.html",
    "title": "Anomaly/outlier detection",
    "section": "",
    "text": "Anomalies are defined as events that deviate from the standard; they occur rarely and do not follow the established pattern.\nIn machine learning, anomaly detection, also known as outlier detection, is the identification of rare data points which raise suspicions by differing significantly from the majority of the data. Anomalies can be indicative of issues such as bank fraud, structural defects, medical problems, or errors in a text.\nExamples of anomalies include:\n\nreal-word anomalies:\n\nLarge dips and spikes in the stock market due to world events\nDefective items in a factory/on a conveyor belt\nContaminated samples in a lab\n\nmachine learning applications:\n\nIdentifying unusual patterns that might indicate fraudulent activity\nDetecting anomalies in patient data that could indicate medical issues\nUnsupervised vs Supervised Detection:\n\nUnsupervised Detection: Most common in anomaly detection. Techniques include autoencoders and clustering algorithms, for scenario when the nature of anomalies is not known a priori.\nSupervised Detection: Classification models can be trained to detect anomalies with labeled data. It‚Äôs is not a recommended for real-world data is more noisy and human can miss some aspects of data when labeling it.\nSemi-supervised Learning: Involves training on a large amount of unlabeled data and a small amount of labeled data."
  },
  {
    "objectID": "posts/outl.html#what-are-anomaly-and-outlier",
    "href": "posts/outl.html#what-are-anomaly-and-outlier",
    "title": "Anomaly/outlier detection",
    "section": "",
    "text": "Anomalies are defined as events that deviate from the standard; they occur rarely and do not follow the established pattern.\nIn machine learning, anomaly detection, also known as outlier detection, is the identification of rare data points which raise suspicions by differing significantly from the majority of the data. Anomalies can be indicative of issues such as bank fraud, structural defects, medical problems, or errors in a text.\nExamples of anomalies include:\n\nreal-word anomalies:\n\nLarge dips and spikes in the stock market due to world events\nDefective items in a factory/on a conveyor belt\nContaminated samples in a lab\n\nmachine learning applications:\n\nIdentifying unusual patterns that might indicate fraudulent activity\nDetecting anomalies in patient data that could indicate medical issues\nUnsupervised vs Supervised Detection:\n\nUnsupervised Detection: Most common in anomaly detection. Techniques include autoencoders and clustering algorithms, for scenario when the nature of anomalies is not known a priori.\nSupervised Detection: Classification models can be trained to detect anomalies with labeled data. It‚Äôs is not a recommended for real-world data is more noisy and human can miss some aspects of data when labeling it.\nSemi-supervised Learning: Involves training on a large amount of unlabeled data and a small amount of labeled data."
  },
  {
    "objectID": "posts/outl.html#anomaly-detection-in-computer-vision",
    "href": "posts/outl.html#anomaly-detection-in-computer-vision",
    "title": "Anomaly/outlier detection",
    "section": "üíú Anomaly detection in computer vision",
    "text": "üíú Anomaly detection in computer vision\nAnomaly detection in computer vision focuses on identifying abnormal patterns or objects in visual data that do not conform to expected norms.\n\nIsolation Forest is a type of ensemble algorithm and consist of multiple decision trees used to partition the input dataset into distinct groups of inliers. It is good for high-dimensional data like image data and is based on the principle that anomalies data points that are ‚Äúisolated‚Äù in the sense that they are rare and different in terms of pixel values.\nLet‚Äôs try if a human is better at distinguish cats than a ‚Äúrobot‚Äù using the Isolation Forest. Anomalies will be those points that have a shorter average path length in the trees of the forest.\nWe load the cats_vs_dogs dataset to start.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nfrom datasets import load_dataset\nfrom PIL import Image\n\n# Load the dataset\ndataset = load_dataset('Bingsu/Cat_and_Dog')  \n\n\nPreprocess the dataset to get a majority of dogs and a few of cats as anomalies.\n\n\nCode\nfrom datasets import concatenate_datasets\ndata = dataset['train']\ncats = data.filter(lambda x: x['labels']==0)\ndogs = data.filter(lambda x: x['labels']==1)\ndata = concatenate_datasets([cats.shuffle(seed=1).select(range(100)), dogs.shuffle(seed=0).select(range(900))])\ndata = data.shuffle(seed=0)\n\n\nWe also preprocess the images to get array representations of pixels for fitting the model.\n\n\nCode\ndef preprocess_image(image):\n    # convert the input image to grayscale\n    #image = image.convert('L')\n    image = image.resize((64, 64))  # Resize to 64x64\n    return np.array(image).flatten()  # Flatten the image\n\n# Preprocess the images\nimages = np.array([preprocess_image(item['image']) for item in data])\n\n\nApply Isolation Forest to the dogs dataset and plot out some anomalies to see if they are cats!\n\n\nCode\n# Applying Isolation Forest\niso_forest = IsolationForest(n_estimators=100, contamination=0.1)  # contamination is an estimate of the proportion of outliers\nanomalies = iso_forest.fit_predict(images)\n\n# Identifying the indices of anomalies\nanomaly_indices = np.where(anomalies == -1)[0]\n\n# Plot some of the detected anomalies\nfig, axs = plt.subplots(1, 4, figsize=(15, 3))\nfor i, idx in enumerate(anomaly_indices[:4]):\n    axs[i].imshow(images[idx].reshape(64, 64,3), cmap='gray')\n    axs[i].axis('off')\n    axs[i].set_title(f'Anomaly {i+1}')\n\nplt.show()\n\n\n\n\n\nWe can see the detection is not very good, the model seems to only capture the black and white color as the anomalies, which might not always hold true for all types of anomalies in image data.\nThus, we could try some more complex models such as Autoencoders, which use neural networks designed to compress and then reconstruct input data, often used to detect anomalies by comparing the reconstruction error."
  },
  {
    "objectID": "posts/outl.html#outliers-from-hdbscan",
    "href": "posts/outl.html#outliers-from-hdbscan",
    "title": "Anomaly/outlier detection",
    "section": "‚ù§Ô∏è Outliers from HDBSCAN",
    "text": "‚ù§Ô∏è Outliers from HDBSCAN\nWe looked at twitter topic modeling with HDBSCAN clustering in the Clustering blog. We removed outliers at the time, now let‚Äôs plot the outliers too. Cluster -1 in HDBSCAN clusters is the collection of outliers.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nimport hdbscan\n\n# For plotting\nfrom matplotlib import offsetbox\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\n\ndataset = load_dataset(\"cardiffnlp/tweet_topic_single\", split=\"train_2021\")\nembeddings = OpenAIEmbeddings(chunk_size=1000).embed_documents(dataset[\"text\"])\n\ntensor=np.array(embeddings)\n\ntsne = TSNE(random_state = 1, n_components=2,verbose=0).fit_transform(tensor)\n\ncluster = hdbscan.HDBSCAN(min_cluster_size=20, prediction_data=True).fit(tsne)\n\ndf = pd.DataFrame({\n    \"tweet\": dataset[\"text\"],\n    \"label\": dataset[\"label_name\"],\n    \"cluster\": cluster.labels_,\n})\n\nplt.scatter(tsne[:, 0], tsne[:, 1],s=5, c=df[\"cluster\"])\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar()\nplt.title('Tweets topic t-SNE', fontsize=20)\n\n\nText(0.5, 1.0, 'Tweets topic t-SNE')\n\n\n\n\n\nLet‚Äôs take a look at what kind of tweets are outliers\n\n\nCode\ndf[df[\"cluster\"] == -1].head(5)\n\n\n\n\n\n\n\n\n\ntweet\nlabel\ncluster\n\n\n\n\n0\nBarbara Bailey is the queen of broadcast news ...\npop_culture\n-1\n\n\n1\nstart the 20-21 school year off POSITIVE! let‚Äô...\ndaily_life\n-1\n\n\n2\nWorth watching at least the 1st 10mins if I wa...\npop_culture\n-1\n\n\n3\nThere s regular people and then there s {@Bail...\npop_culture\n-1\n\n\n4\nUp with new grace, truly sorry on behalf of {@...\npop_culture\n-1\n\n\n\n\n\n\n\n\nRef\nIsolation-forest"
  },
  {
    "objectID": "posts/line.html",
    "href": "posts/line.html",
    "title": "Linear and non-linear regression",
    "section": "",
    "text": "Regression is supervised  learning algorithm, for each input has a corresponding output, and algorithms are trained to predict the output based on the input. Linear regression models the linear relationship between a dependent variable \\(Y\\) (response variable) and one or more independent variables \\(X_i\\) (predictors). It‚Äôs used to predict values within a continuous range, like housing price.\nVanilla linear regression involves only one independent variable X. The model takes the linear function:\n\\[\nY = \\beta_0 + \\beta_1X + \\epsilon\n\\]\n\n\\(\\beta_0\\) is the y-intercept constant\n\\(\\beta_1\\) is the slope coefficient for \\(X\\)\n\\(\\epsilon\\) is the error term (residuals, representing the part of \\(Y\\) not explained by \\(X\\))"
  },
  {
    "objectID": "posts/line.html#what-is-linear-regression",
    "href": "posts/line.html#what-is-linear-regression",
    "title": "Linear and non-linear regression",
    "section": "",
    "text": "Regression is supervised  learning algorithm, for each input has a corresponding output, and algorithms are trained to predict the output based on the input. Linear regression models the linear relationship between a dependent variable \\(Y\\) (response variable) and one or more independent variables \\(X_i\\) (predictors). It‚Äôs used to predict values within a continuous range, like housing price.\nVanilla linear regression involves only one independent variable X. The model takes the linear function:\n\\[\nY = \\beta_0 + \\beta_1X + \\epsilon\n\\]\n\n\\(\\beta_0\\) is the y-intercept constant\n\\(\\beta_1\\) is the slope coefficient for \\(X\\)\n\\(\\epsilon\\) is the error term (residuals, representing the part of \\(Y\\) not explained by \\(X\\))"
  },
  {
    "objectID": "posts/line.html#diabetes-progression",
    "href": "posts/line.html#diabetes-progression",
    "title": "Linear and non-linear regression",
    "section": "üíú Diabetes progression",
    "text": "üíú Diabetes progression\nLet‚Äôs use linear regression on a simple example, picking one attribute from the diabetes dataset as independent variable. The target variable is a quantitative measure of disease progression one year after baseline.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\nPerform simple linear regression and evaluate the results\n\n\nCode\n# Load the diabetes dataset\ndiabetes = datasets.load_diabetes()\nX = diabetes.data[:, np.newaxis, 9]  # Use only one feature\ny = diabetes.target\n\n# Split the data into training/testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create linear regression object\nregr = LinearRegression()\n\n# Train the model using the training sets\nregr.fit(X_train, y_train)\n\n# Make predictions using the testing set\ny_pred = regr.predict(X_test)\n\n# The coefficients\nprint('Coefficients: \\n', regr.coef_)\n# The mean squared error\nprint('Mean squared error: %.2f' % mean_squared_error(y_test, y_pred))\n# The coefficient of determination: 1 is perfect prediction\nprint('R2 Score: %.2f' % r2_score(y_test, y_pred))\n\n# Plot outputs\nplt.scatter(X_test, y_test, color='black')\nplt.plot(X_test, y_pred, color='blue', linewidth=3)\nplt.xlabel('Feature')\nplt.ylabel('Target')\nplt.title('Simple Linear Regression')\nplt.show()\n\n\nCoefficients: \n [629.85314634]\nMean squared error: 4715.16\nR2 Score: 0.11"
  },
  {
    "objectID": "posts/line.html#non-linear-regression",
    "href": "posts/line.html#non-linear-regression",
    "title": "Linear and non-linear regression",
    "section": "‚ù§Ô∏è Non-linear regression",
    "text": "‚ù§Ô∏è Non-linear regression\nDifferent from linear regression, where the model is a straight line, non-linear regression have models that bend or curve. This flexibility makes it suitable for many complex real-world situations where the relationship between variables is not linear. The relationship between the dependent variable and one or more independent variables is modeled with a non-linear function. Common examples include exponential, logarithmic, and polynomial models.\nPolynomial regression involves independent variable X. The model takes the linear function:\n\\[\ny = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3 + \\cdots + \\beta_n x^n + \\epsilon\n\\]\n\n\\(\\beta_0\\), \\(\\beta_1,\\cdots\\), \\(\\beta_n\\) are the coefficients of the model\n\\(n\\) is the degree of the polynomial\n\\(\\epsilon\\) is the error term (residuals, representing the part of \\(Y\\) not explained by \\(X\\))\n\n\n\nCode\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Polynomial Regression Model\ndegree = 2  # Degree of the polynomial\npoly_features = PolynomialFeatures(degree=degree)\nX_train_poly = poly_features.fit_transform(X_train)\nX_test_poly = poly_features.transform(X_test)\n\n# Linear regression model\nmodel = LinearRegression()\nmodel.fit(X_train_poly, y_train)\ny_pred = model.predict(X_test_poly)\n\n# Evaluation\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\nprint(f\"Mean Squared Error: {mse:.2f}\")\nprint(f\"R2 Score: {r2:.2f}\")\n\n# Plotting\nplt.scatter(X_test, y_test, color='gray')\nplt.scatter(X_test, y_pred, color='red', linewidth=2)\nplt.title('Polynomial Regression (Degree = 2)')\nplt.xlabel('Feature')\nplt.ylabel('Diabetes Progression')\nplt.show()\n\n\nMean Squared Error: 4606.71\nR2 Score: 0.13\n\n\n\n\n\nCompare to the previous linear regression model, the polynomial regression model has a lower Mean Squared Error which means better performance.\nMean Squared Error(MSE): \\(\\sum_{i=1}^{D}(x_i-y_i)^2\\)\nThe smaller the Mean Squared Error, the closer you are to finding the line of best fit.\nR-squared: It is a statistical measure of how close the data points are to the fitted regression line. Its value is always between 0 and 1. The closer to 1, the better the regression model fits the observations.\n\nRef\nIntro-1"
  },
  {
    "objectID": "posts/clas.html",
    "href": "posts/clas.html",
    "title": "Classification",
    "section": "",
    "text": "Classification is supervised  learning algorithm, where the model learns from a training dataset that has paired input data with their correct output labels. They are many types of clustering methods, let‚Äôs take a look at some examples:\n\nK-Nearest Neighbours (kNN): calculate the distance of one data point from every other data point and then takes a majority vote from k-nearest neighbors of each data points to classify the output.\nDecision trees: use multiple if-else statements in the form of a tree structure that includes nodes and leaves. The nodes breaking down the one major structure into smaller structures and eventually providing the final outcome.\nRandom Forest: uses multiple decision trees to predict the outcome of the target variable. Each decision tree provides its own outcome and then it takes the majority vote to classify the final outcome.\nSupport Vector Machines: it creates an n-dimensional space for the n number of features in the dataset and then tries to create the hyperplanes such that it divides and classifies the data points with the maximum margin possible."
  },
  {
    "objectID": "posts/clas.html#what-is-classification",
    "href": "posts/clas.html#what-is-classification",
    "title": "Classification",
    "section": "",
    "text": "Classification is supervised  learning algorithm, where the model learns from a training dataset that has paired input data with their correct output labels. They are many types of clustering methods, let‚Äôs take a look at some examples:\n\nK-Nearest Neighbours (kNN): calculate the distance of one data point from every other data point and then takes a majority vote from k-nearest neighbors of each data points to classify the output.\nDecision trees: use multiple if-else statements in the form of a tree structure that includes nodes and leaves. The nodes breaking down the one major structure into smaller structures and eventually providing the final outcome.\nRandom Forest: uses multiple decision trees to predict the outcome of the target variable. Each decision tree provides its own outcome and then it takes the majority vote to classify the final outcome.\nSupport Vector Machines: it creates an n-dimensional space for the n number of features in the dataset and then tries to create the hyperplanes such that it divides and classifies the data points with the maximum margin possible."
  },
  {
    "objectID": "posts/clas.html#multinomial-regression",
    "href": "posts/clas.html#multinomial-regression",
    "title": "Classification",
    "section": "üíú Multinomial Regression",
    "text": "üíú Multinomial Regression\nLogistic regression is used for classification of binary variables. Multinomial Regression also known as softmax function, is an extension of Logistic Regression (sigmoid function) to a higher number of classes.\n\nThe simplest approach to multinomial data is to nominate one of the response categories as a baseline or reference cell, calculate log-odds for all other categories relative to the baseline, and then let the log-odds be a linear function of the predictors.\nWe can do simultaneously estimation on multiple binary logit models with two given classes: \\(ln\\left(\\frac{\\pi_{ij}}{\\pi_{ib}}\\right) = ln\\left(\\frac{P(y_i=j|x)}{P(y_i=b|x)}\\right)=x_i \\beta_j\\)\nWhen j = b, the left side of the equation becomes \\(ln(1)=0\\), and \\(\\beta_b=0\\), the class b would have log-odds of 0 compare with itself. Typically, we pick the last category as a baseline and calculate the odds that a member of group i falls in category j as opposed to the baseline.\nLet‚Äôs use it on a simple example, knowing the sepal and petal‚Äôs length and width, will the softmax model predict the iris species?\n\n\n\niris images\n\n\nFirst, we take a look at the dataset\n\n\nCode\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix,classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the iris dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target \n\niris_df = pd.DataFrame(iris.data,columns=iris.feature_names)\niris_df['target']=iris.target\niris_df.head(5)\n\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0\n\n\n\n\n\n\n\nWe can see some differences between iris classes\n\n\nCode\nsns.pairplot(iris_df, hue='target', height=1.5, aspect=1)\n\n\n\n\n\n¬†\n\nTrain the model and make predictions, our model gets üëÄ\nA very good performance ~\n\n\nCode\n# Split the dataset into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n\nmodel = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=100)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(\"Accuracy:\", accuracy)\n\n\nAccuracy: 0.9733333333333334\n\n\nConfusion matrix provides a visual and numerical representation of the model‚Äôs performance, showing how many instances of each actual class were predicted as each predicted class. It‚Äôs useful for identifying any particular classes that the model might be confusing with others.\nPlotting the confusion matrix to see where the model got wrong\n\n\nCode\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap='Purples', xticklabels=iris.target_names, yticklabels=iris.target_names)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Confusion Matrix')\nplt.show()\n\n\n\n\n\nFinally, we can kinda see the softmax curve of three species through a 3D plot\n\n\nCode\nimport plotly.graph_objects as go\n\n# Compute the probabilities\ny_probas = model.predict_proba(X_test)\n\n# Creating a DataFrame for the test dataset with probabilities\ntest_df = pd.DataFrame(X_test, columns=iris.feature_names)\ntest_df['Prob_Setosa'] = y_probas[:, 0]\ntest_df['Prob_Versicolour'] = y_probas[:, 1]\ntest_df['Prob_Virginica'] = y_probas[:, 2]\n\nfig = go.Figure()\nfor feat in ['Setosa', 'Versicolour', 'Virginica']:\n    fig.add_trace(go.Scatter3d(x=test_df['sepal length (cm)'], y=test_df['sepal width (cm)'], z=test_df[f'Prob_{feat}'], mode='markers', name=feat))\nfig.update_traces(marker=dict(size=3))\nfig.update_layout(\n    title={\n        'text': \"Probabilities of Species\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\nfig.show()"
  },
  {
    "objectID": "posts/clas.html#classification-evaluation-metrics",
    "href": "posts/clas.html#classification-evaluation-metrics",
    "title": "Classification",
    "section": "Classification evaluation metrics",
    "text": "Classification evaluation metrics\nFirst thing came to mind is our old friend: accuracy\n\nAccuracy = \\(\\frac{TP + TN}{TP + TN + FP + FN}\\)\n\nWait a second, what are all these, isn‚Äôt it just corrects over all?\nIt is! Just with different concepts which are very important for the following metrics\n\nTrue Positives (TP): The model correctly predicted the positive class.\nTrue Negatives (TN): The model correctly predicted the negative class.\nFalse Positives (FP): The model incorrectly predicted the positive class (also known as ‚ÄúType I error‚Äù).\nFalse Negatives (FN): The model incorrectly predicted the negative class (also known as ‚ÄúType II error‚Äù).\n\nWow, so many names!\n\nPrecision = \\(\\frac{TP}{TP + FP}\\)\nRecall = \\(\\frac{TP}{TP + FN}\\) (True Positive Rate (TPR), Sensitivity or Recall)\nF1 Score = \\(2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\)\nMacro F1 Score = \\(\\frac{1}{N} \\sum_{i=1}^{N} F1_i\\)\n\n(treat all classes equally when the dataset is imbalanced)\n\nMicro F1 Score = \\(2 \\times \\frac{\\text{Total TP}}{\\text{Total TP} + \\text{Total FP} + \\text{Total FN}}\\)\n\n(weigh each instance equally when the dataset is imbalanced)\n\nSpecificity = \\(\\frac{TN}{TN + FP}\\)\nFalse Positive Rate (FPR) = \\(\\frac{FP}{TN + FP}\\)\nROC curve (receiver operating characteristic curve) plots TPR vs.¬†FPR at different classification thresholds.\nAUC (Area under the ROC Curve) provides a single measure of overall performance of a classification model from 0 to 1.\n\n\n\nCode\nfrom sklearn.metrics import roc_curve, auc\nimport scikitplot as skplt\n# Plot ROC curve and AUC for each class\nskplt.metrics.plot_roc(y_test, y_probas)\nplt.show()\n\n\n\n\n\nArea = 1 shows it‚Äôs a perfect classifier even some other metrics don‚Äôt agree\n\n\nCode\nprint(classification_report(y_test, y_pred))\n\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        24\n           1       0.92      1.00      0.96        24\n           2       1.00      0.93      0.96        27\n\n    accuracy                           0.97        75\n   macro avg       0.97      0.98      0.97        75\nweighted avg       0.98      0.97      0.97        75\n\n\n\n\nRef\nIntro-1 ploty roc"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "I‚Äôm a graduate student researcher with a deep-seated passion for AI and ML. Armed with dual bachelor‚Äôs degrees in Computer Science and Data Science, I‚Äôm currently advancing my expertise through an MS in CS focusing on NLP and multimodal learning."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS 5805 Final Project",
    "section": "",
    "text": "Anomaly/outlier detection\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear and non-linear regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProbability theory and random variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/clus.html",
    "href": "posts/clus.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is unsupervised  machine learning for not having a target variable or class label. Clustering takes unlabeled data as input and group them into several clusters based on certain similarities. They are many types of clustering methods, let‚Äôs take a look at some examples:\n\n\n\nK-Means Clustering: The most popular partitioning method. It divides the dataset into K clusters by minimizing the sum of squared distances between data points and their respective cluster centroids. First, you need to select the number of clusters. The algorithm then iteratively improves the clusters by recalculating the centroid of each cluster and reassigning data points to the new clusters.\n\n\n\n\n\nFuzzy C-Means: Similar to K-means but allows data points to belong to multiple clusters with varying degrees of membership. The algorithm depends on a parameter m which corresponds to the degree of fuzziness of the solution. Large values of m will blur the classes and all elements tend to belong to all clusters.\n\n\n\n\nDifference between hard clustering and soft clustering: Hard clustering means that each data point is assigned to a specific cluster, and soft clustering means that each data point is assigned a probability of belonging to each cluster.\n\n\n\n\n\n\nSelf-Organizing Maps: A self-organizing map consists of a set of neurons that are arranged in a rectangular or hexagonal grid. Each neuronal unit in the grid is associated with a numerical vector of fixed dimensionality. The learning process of a self-organizing map involves the adjustment of these vectors to provide a suitable representation of the input data. Neural networks that produce a low-dimensional (typically two-dimensional) representation of the training data, preserving topological properties. Self-organizing maps can be used for clustering numerical data in vector format.\n\n\n\n\nPartitioning clustering is a method of clustering data points into a set number of groups, while hierarchical clustering is a method of creating a hierarchy of clusters, with each cluster containing a subset of the data points. Partitioning clustering is typically faster than hierarchical clustering, but hierarchical clustering can produce more accurate results.\n\nAgglomerative: A bottom-up approach where each data point starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\nDivisive: A top-down approach starting with all points in one cluster and recursively splitting them into smaller clusters.\n\n\n\n\n\nMean Shift: A sliding-window-based method that tries to find dense areas by updating candidates for centroids to be the mean of the points within a given region.\n\nWe can understand this algorithm by thinking of our data points to be represented as a probability density function. Naturally, in a probability function, higher density regions will correspond to the regions with more points, and lower density regions will correspond to the regions with less points. In clustering, we need to find clusters of points, i.e the regions with a lot of points together. More points together mean higher density. Hence, we observe that clusters of points are more like the higher density regions in our probability density function. So, we must iteratively go from lower density to higher density regions, in order to find our clusters.\n\n\n\nDifferent clustering algorithms on different shapes of data"
  },
  {
    "objectID": "posts/clus.html#what-is-clustering",
    "href": "posts/clus.html#what-is-clustering",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is unsupervised  machine learning for not having a target variable or class label. Clustering takes unlabeled data as input and group them into several clusters based on certain similarities. They are many types of clustering methods, let‚Äôs take a look at some examples:\n\n\n\nK-Means Clustering: The most popular partitioning method. It divides the dataset into K clusters by minimizing the sum of squared distances between data points and their respective cluster centroids. First, you need to select the number of clusters. The algorithm then iteratively improves the clusters by recalculating the centroid of each cluster and reassigning data points to the new clusters.\n\n\n\n\n\nFuzzy C-Means: Similar to K-means but allows data points to belong to multiple clusters with varying degrees of membership. The algorithm depends on a parameter m which corresponds to the degree of fuzziness of the solution. Large values of m will blur the classes and all elements tend to belong to all clusters.\n\n\n\n\nDifference between hard clustering and soft clustering: Hard clustering means that each data point is assigned to a specific cluster, and soft clustering means that each data point is assigned a probability of belonging to each cluster.\n\n\n\n\n\n\nSelf-Organizing Maps: A self-organizing map consists of a set of neurons that are arranged in a rectangular or hexagonal grid. Each neuronal unit in the grid is associated with a numerical vector of fixed dimensionality. The learning process of a self-organizing map involves the adjustment of these vectors to provide a suitable representation of the input data. Neural networks that produce a low-dimensional (typically two-dimensional) representation of the training data, preserving topological properties. Self-organizing maps can be used for clustering numerical data in vector format.\n\n\n\n\nPartitioning clustering is a method of clustering data points into a set number of groups, while hierarchical clustering is a method of creating a hierarchy of clusters, with each cluster containing a subset of the data points. Partitioning clustering is typically faster than hierarchical clustering, but hierarchical clustering can produce more accurate results.\n\nAgglomerative: A bottom-up approach where each data point starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\nDivisive: A top-down approach starting with all points in one cluster and recursively splitting them into smaller clusters.\n\n\n\n\n\nMean Shift: A sliding-window-based method that tries to find dense areas by updating candidates for centroids to be the mean of the points within a given region.\n\nWe can understand this algorithm by thinking of our data points to be represented as a probability density function. Naturally, in a probability function, higher density regions will correspond to the regions with more points, and lower density regions will correspond to the regions with less points. In clustering, we need to find clusters of points, i.e the regions with a lot of points together. More points together mean higher density. Hence, we observe that clusters of points are more like the higher density regions in our probability density function. So, we must iteratively go from lower density to higher density regions, in order to find our clusters.\n\n\n\nDifferent clustering algorithms on different shapes of data"
  },
  {
    "objectID": "posts/clus.html#common-tasks-and-real-world-examples",
    "href": "posts/clus.html#common-tasks-and-real-world-examples",
    "title": "Clustering",
    "section": "üíú Common tasks and real-world examples",
    "text": "üíú Common tasks and real-world examples\nClustering can be used for a variety of tasks, such as\n\nRecommendation systems: Recommendation systems group similar items together and discover underlying connections without pre-defined categories. It recommends similar content and identifies customer segments for marketing purposes.\nSearch engines: Search engines try to group related objects together in a cluster and place different things far apart. It produces search results for the desired data based on the closest comparable items which are grouped around the desired search standards.\nTopic modeling: Topic modeling is a method used in text mining to discover abstract topics within a collection of text crops. Perform clustering on the reduced text feature space to get groups of text crops, and each cluster can be interpreted as a topic."
  },
  {
    "objectID": "posts/clus.html#hdbscan",
    "href": "posts/clus.html#hdbscan",
    "title": "Clustering",
    "section": "‚ù§Ô∏è HDBSCAN",
    "text": "‚ù§Ô∏è HDBSCAN\nHDBSCAN stands for Hierarchical Density-Based Spatial Clustering of Applications with Noise, which is a a density based (hierarchical) clustering algorithm. HDBSCAN is a versatile clustering algorithm that can handle complex real-world data sets with varying densities and noise. Therefore it‚Äôs used in a wide range of domains, especially for high-dimensional data such as image clustering, anomaly detection, and topic modeling, for the ability to handle different data sizes and shapes and pre-select the number of clusters.\nIn the following parts, I will share an example of using HDBSCAN for topic modeling. First, let‚Äôs retrieve some Twitter post data from Huggingface and use OpenAI to obtain the embeddings for clustering.\n\n\nCode\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"cardiffnlp/tweet_topic_single\", split=\"train_2021\")\nembeddings = OpenAIEmbeddings(chunk_size=1000).embed_documents(dataset[\"text\"])\n\n\nCheck the size of embeddings:\n\n\nCode\ntensor=np.array(embeddings)\ntensor.shape\n\n\n(1516, 1536)\n\n\nSince the embeddings are of very high dimensions, we will perform a simple dimension reduction technique.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\n# For plotting\nfrom matplotlib import offsetbox\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(random_state = 1, n_components=2,verbose=0).fit_transform(tensor)\ntsne.shape\n\n\n(1516, 2)\n\n\nNext, we preprocess the data into a cleaner dataframe and apply HDBSCAN to this 2-dimensional data to obtain the cluster labels.\n\n\nCode\ncluster = hdbscan.HDBSCAN(min_cluster_size=20, prediction_data=True).fit(tsne)\n\ndf = pd.DataFrame({\n    \"tweet\": dataset[\"text\"],\n    \"label\": dataset[\"label_name\"],\n    \"cluster\": cluster.labels_,\n    \"tsne_1\": tsne[:, 0],\n    \"tsne_2\": tsne[:, 1]\n})\n\ndf = df[df[\"cluster\"] != -1] #remove outliers\n\ndf.head(5)\n\n\n\n\n\n\n\n\n\ntweet\nlabel\ncluster\ntsne_1\ntsne_2\n\n\n\n\n0\nBarbara Bailey is the queen of broadcast news ...\npop_culture\n2\n2.815385\n-4.618085\n\n\n3\nThere s regular people and then there s {@Bail...\npop_culture\n2\n3.692987\n-4.774335\n\n\n6\nPOWER BOOK 2 EPISODE 1 REVIEW!! (MY THOUGHTS) ...\npop_culture\n2\n5.896851\n17.536180\n\n\n7\nyou cannot simultaneously prepare and prevent...\npop_culture\n2\n10.823426\n11.094867\n\n\n8\nThe time is now! The 1st edition of the 192 ...\narts_&_culture\n2\n6.077450\n8.689940\n\n\n\n\n\n\n\nWe can use ChatGPT to understand our clusters better by prompting ChatGPT to provide keywords and a short description for each cluster.\n\n\nCode\nfrom openai import OpenAI\n\nclient = OpenAI()\n\ndef cluster_summary(df, sample_size=3):\n\n    summary = []\n    for i in tqdm(df['cluster'].unique()):\n        sample = df[df['cluster']==i].sample(sample_size, random_state=1)\n        s=sample[[\"tweet\"]].to_dict(orient=\"records\")\n        print(f'\\nAsking the LLM for a summary for cluster indexed {i}. \\n')\n        content = f\"\"\"You will be given a dataset with some tweets. You need to identify the category for those given tweets and shortly describe it. Tweets: {s}\"\"\"\n        \n        print(\"Ground truth labels: \"+ \", \".join(sample[\"label\"].to_list()))\n        \n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\"role\": \"user\", \"content\": content}],\n            temperature=0.5,\n            max_tokens=64,\n            top_p=1\n        )\n        print(response.choices[0].message.content)\n\n        summary.append(response)\n\ncluster_summary(df,3)\n\n\n  0%|          | 0/5 [00:00&lt;?, ?it/s] 20%|‚ñà‚ñà        | 1/5 [00:02&lt;00:10,  2.59s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:04&lt;00:07,  2.40s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:07&lt;00:04,  2.40s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:09&lt;00:02,  2.45s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:12&lt;00:00,  2.40s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:12&lt;00:00,  2.42s/it]\n\n\n\nAsking the LLM for a summary for cluster indexed 2. \n\nGround truth labels: business_&_entrepreneurs, daily_life, daily_life\nThe first tweet is related to job hiring and marketing opportunities. It mentions the company's achievements in November and highlights the potential for a great storyteller. The tweet invites interested individuals to inquire for more details.\n\nThe second tweet conveys a message about finding joy in the small things in life. It mentions a person named Kyle\n\nAsking the LLM for a summary for cluster indexed 3. \n\nGround truth labels: pop_culture, pop_culture, pop_culture\nBased on the given tweets, the categories for each tweet can be described as follows:\n\n1. Tweet 1: Music Appreciation\n   Description: This tweet is expressing appreciation for the music crew on NBC Entertainment's show, specifically mentioning the artist \"Drivin N Cryin.\" The hashtag \"#aintitstrange\n\nAsking the LLM for a summary for cluster indexed 1. \n\nGround truth labels: sports_&_gaming, sports_&_gaming, sports_&_gaming\nCategory: Sports\n\nDescription: The given tweets are related to sports. The first tweet discusses an NFLDraft conspiracy theory, reviews the Houston Sports Awards, and includes an interview with Gary Kubiak. The second tweet mentions a 21-year-old Southampton left-back likely leaving the club on loan. The third tweet features an\n\nAsking the LLM for a summary for cluster indexed 0. \n\nGround truth labels: sports_&_gaming, sports_&_gaming, sports_&_gaming\nThe first tweet is about a boxing match between Loma and Teofimo Lopez. The person mentions that Loma started slow but dominated the second half of the fight. They also mention that if Loma had won the final round, it would have been a draw. The category for this tweet could be \"Sports -\n\nAsking the LLM for a summary for cluster indexed 4. \n\nGround truth labels: pop_culture, pop_culture, pop_culture\nThe given tweets belong to the category of BTS fandom. \n\n1. The first tweet expresses the person's liking for the song \"WHO\" by LV, JK & JM, which likely refers to members of BTS (LV - V, JK - Jungkook, JM - Jimin). They mention identifying with the story\n\n\nFinally, let‚Äôs visualize our clusters to see how HDBSCAN works!\n\n\nCode\nplt.scatter(df[\"tsne_1\"], df[\"tsne_2\"],s=5, c=df[\"cluster\"])\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar()\nplt.title('Tweets topic t-SNE', fontsize=20)\n\n\nText(0.5, 1.0, 'Tweets topic t-SNE')\n\n\n\n\n\nWe can also visualize the clustering results without dimention reduction, let‚Äôs see how that works!\n\n\nCode\ncluster = hdbscan.HDBSCAN(min_cluster_size=20, prediction_data=True).fit(embeddings)\n\ndf1 = pd.DataFrame({\n    \"tweet\": dataset[\"text\"],\n    \"label\": dataset[\"label_name\"],\n    \"cluster\": cluster.labels_,\n})\nplt.scatter(tsne[:, 0], tsne[:, 1],s=5, c=df1[\"cluster\"])\nplt.gca().set_aspect('equal', 'datalim')\nplt.colorbar()\nplt.title('Tweets topic t-SNE (without dim-reduction)', fontsize=20)\n\n\nText(0.5, 1.0, 'Tweets topic t-SNE (without dim-reduction)')\n\n\n\n\n\nAlmost all data points are outliers, the clustering doesn‚Äôt work well without dimention reduction. For more about outliers, please check my Anomaly/outlier detection blog!\n\nRef\nIntro-1 Intro-2 Fuzzy-C data hdbscan explain"
  },
  {
    "objectID": "posts/prob.html#probability-theory",
    "href": "posts/prob.html#probability-theory",
    "title": "Probability theory and random variables",
    "section": "üíô Probability theory",
    "text": "üíô Probability theory\nProbability theory analyzes random events and quantifies the likelihood of various outcomes. To understand probability theory, first, we need to define some terms:\n\nSample Space: The set of all possible outcomes of a random experiment. \\(\\Omega\\)\nEvent: A subset of the sample space. \\(E \\subset \\Omega\\)\nProbability: A measure of the likelihood that an event will occur, usually expressed as a number between 0 and 1. The probability of an event E denoted \\(P(E)\\), \\(P(E)\\le0\\) for every E and \\(P(\\Omega)=1\\)\n\nMoreover, we have\n\nJoint Probability: The joint probability of A and B is when event A and B are occurring at the same time, denoted \\(P(A, B)\\)\nConditional Probability: The probability of event A occurring given that event B has already occurred, denoted \\(P(A \\mid B)=\\frac{P(A, B)}{P(B)}\\)\nIndependence: Two events are independent if the occurrence of one does not affect the probability of occurrence of the other.\n\nEvents A and B are independent if \\(P(A, B)=P(A)P(B)\\)\nEvents A and B are conditionally independent given C if \\(P(A, B\\mid C)=P(B \\mid A, C)P(A\\mid C)=P(B\\mid C)P(A \\mid C)\\)"
  },
  {
    "objectID": "posts/prob.html#random-variables",
    "href": "posts/prob.html#random-variables",
    "title": "Probability theory and random variables",
    "section": "üíú Random Variables",
    "text": "üíú Random Variables\nRandom variables are a key concept in probability theory, representing quantities whose values are subject to randomness. Independent and identically distributed (i.i.d.) random variables are those that sampled from the same probability distribution and are mutually independent, for example, coin flips are assumed to be iid.\nRandom Variable: A variable whose value is subject to variations due to chance.\nDiscrete Random Variable: Takes on a countable number of distinct values.\nContinuous Random Variable: Takes on an uncountable range of values."
  },
  {
    "objectID": "posts/prob.html#markov-chain",
    "href": "posts/prob.html#markov-chain",
    "title": "Probability theory and random variables",
    "section": "‚ù§Ô∏è Markov Chain",
    "text": "‚ù§Ô∏è Markov Chain\nA Markov chain is a mathematical system that experiences transitions from one state to another based on the probability theory. It‚Äôs a type of stochastic process that is memoryless, meaning the next state depends only on the current state and not on the sequence of events that preceded it.\n\\(P(X_n = i_n \\mid X_{n-1} = i_{n-1}) = P(X_n = i_n \\mid X_0 = i_0, \\, X_1 = i_1, \\, \\dots, \\, X_{n-1} = i_{n-1})\\)\n\nStates: These are the distinct positions or conditions in which the system can exist. A Markov Chain‚Äôs set of states is often denoted as \\(S\\).\nTransitions: The movement from one state to another. These transitions are governed by probabilities.\nTransition Probability: The probability of moving from one state to another. These probabilities are often represented in a matrix known as the transition matrix.\nTransition Matrix: A square matrix where the element at the \\(i\\)-th row and \\(j\\)-th column represents the probability of moving from state \\(i\\) to state \\(j\\).\nInitial State Distribution: A vector that represents the probabilities of starting in each state.\n\n\nFirst, let‚Äôs take a look at a silly example I made up ‚Äì Think and walk.\nIn this scenario, whenever you go for a walk, each second, you randomly decide whether to think (and stay still) or to walk (and take one step forward). After n seconds, how many steps would you have taken?\n\n\nCode\nstart = 0\nx = []\nn = 50\np=[0.5,0.5]\nfor i in range(n):\n    step = np.random.choice([0,1],p=[0.5,0.5])\n    start = start + step\n    x.append(start)\n\n\nWe simulate the scenario using Markov chain and visualize it.\n\n\nCode\nplt.plot(x)\nplt.xlabel('Time at n seconds',fontsize=10)\nplt.ylabel('Steps',fontsize=10)\nplt.title(\"Think and walk\")\n\n\nText(0.5, 1.0, 'Think and walk')\n\n\n\n\n\nIt looks like we staied at step 1 for a while; let‚Äôs take a look at the distribution of how long we thought at each step.\n\n\nCode\ndata_state = pd.DataFrame({'Steps':x})\ndata_occ = pd.DataFrame(data_state.value_counts('Steps')).rename(columns={0:'Count'})\ndata_occ['Count'] = data_occ['count']\nsns.barplot(x=data_occ.index,y=data_occ['Count'],palette='plasma', hue=data_occ.index)\nplt.ylabel('Seconds of thinking')\n\n\nText(0, 0.5, 'Seconds of thinking')\n\n\n\n\n\nWe can easily prove that the probability of being in a certain state, i.e.¬†an integer number of steps, at time t+1, only depends on the state at time t. This demonstrates the property of the Markov Chain. Now, let‚Äôs think about a more interesting problem but with the same Transition Matrix: ‚ÜôÔ∏è\n\n\nWhat is the expected number of coin flips for getting two consecutive heads?\nWe simulate the problem with 100 rounds of flipping, each round ends at when we get two consecutive heads.\n\n\nCode\nfrom random import random\ndef coin_chain(n):\n    x = []\n    for i in range(n):\n        count = 0\n        steps = 0\n        while count &lt; 2:\n            steps += 1\n            if random() &lt; 0.5:\n                count += 1\n            else:\n                count = 0\n        x.append(steps)\n    return x\n\n\nFlips distribution\n\n\nCode\nx = coin_chain(100)\ndata_state = pd.DataFrame({'Number of rounds':x})\ndata_occ = pd.DataFrame(data_state.value_counts('Number of rounds')).rename(columns={0:'Count'})\ndata_occ['Count'] = data_occ['count']\nsns.barplot(x=data_occ.index,y=data_occ['Count'],palette='plasma',hue=data_occ.index)\nplt.ylabel('Flip Count')\n\n\nText(0, 0.5, 'Flip Count')\n\n\n\n\n\nAverage of flips:\n\n\nCode\nnp.mean(x)\n\n\n6.06\n\n\nIncrease the number of rounds to see what‚Äôs the average of filps we get:\n\n\nCode\nx = coin_chain(1000)\ndata_state = pd.DataFrame({'Number of rounds':x})\ndata_occ = pd.DataFrame(data_state.value_counts('Number of rounds')).rename(columns={0:'Count'})\ndata_occ['Count'] = data_occ['count']\nsns.barplot(x=data_occ.index,y=data_occ['Count'],palette='plasma',hue=data_occ.index)\nplt.ylabel('Flip Count')\n\n\nText(0, 0.5, 'Flip Count')\n\n\n\n\n\nAverage of flips:\n\n\nCode\nnp.mean(x)\n\n\n5.946\n\n\nEven more rounds!\n\n\nCode\nx = coin_chain(5000)\ndata_state = pd.DataFrame({'Number of rounds':x})\ndata_occ = pd.DataFrame(data_state.value_counts('Number of rounds')).rename(columns={0:'Count'})\ndata_occ['Count'] = data_occ['count']\nsns.barplot(x=data_occ.index,y=data_occ['Count'],palette='plasma',hue=data_occ.index)\nplt.ylabel('Flip Count')\n\n\nText(0, 0.5, 'Flip Count')\n\n\n\n\n\nAverage of flips:\n\n\nCode\nnp.mean(x)\n\n\n6.0664\n\n\nWe can see that the answer is 6 rounds. The simulation process used here is known as Monte Carlo simulation, which is a very important technique in machine learning.\n\n\nRef\nIntro-1 markov-chains Random walk Monty Hall"
  }
]